{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/recbole/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from sklearn import preprocessing\n",
    "\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "\n",
    "dpath = '../ml-100k/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def age_map(x):\n",
    "    x = int(x)\n",
    "    if x < 20:\n",
    "        return '10'\n",
    "    elif x >= 20 and x < 30:\n",
    "        return '20'\n",
    "    elif x >= 30 and x < 40:\n",
    "        return '30'\n",
    "    elif x >= 40 and x < 50:\n",
    "        return '40'\n",
    "    elif x >= 50 and x < 60:\n",
    "        return '50'\n",
    "    else:\n",
    "        return '60'\n",
    "\n",
    "df = pd.read_csv(os.path.join(dpath,'u.data'), sep='\\t', header=None)\n",
    "df.columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "user2idx = {j:i for i,j in enumerate(df.user_id.unique())}\n",
    "item2idx = {j:i for i,j in enumerate(df.item_id.unique())}\n",
    "\n",
    "df['user_id'] = df['user_id'].map(user2idx)\n",
    "df['item_id'] = df['item_id'].map(item2idx)\n",
    "\n",
    "movies_df = pd.read_csv(os.path.join(dpath,'u.item'), sep='|', header=None, encoding='latin-1')\n",
    "movies_df.columns = ['movie_id', 'movie_title', 'release_date', 'video_release_date',\n",
    "                    'IMDb_URL', 'unknown', 'Action', 'Adventure', 'Animation', \n",
    "                    'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    "                    'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi',\n",
    "                    'Thriller', 'War', 'Western']\n",
    "\n",
    "users_df = pd.read_csv(os.path.join(dpath,'u.user'), sep='|', encoding='latin-1', header=None)\n",
    "users_df.columns = ['user_id', 'age', 'gender', 'occupation', 'zip_code']\n",
    "\n",
    "users_df['age'] = users_df['age'].apply(age_map)\n",
    "\n",
    "movies_df.drop(['movie_title', 'release_date', 'video_release_date', 'IMDb_URL'], axis=1, inplace=True)\n",
    "movies_df['movie_id'] = movies_df['movie_id'].map(item2idx)\n",
    "users_df['user_id'] = users_df['user_id'].map(user2idx)\n",
    "\n",
    "df.rename(columns={'item_id':'movie_id'}, inplace=True)\n",
    "\n",
    "df = pd.merge(df, movies_df,how='left', on = 'movie_id')\n",
    "df = pd.merge(df, users_df, how='left',on = 'user_id')\n",
    "\n",
    "df.drop(['timestamp', 'zip_code'], axis=1, inplace=True)\n",
    "le = preprocessing.LabelEncoder() \n",
    "df['gender'] = le.fit_transform(df['gender'])\n",
    "df['age'] = le.fit_transform(df['age'])\n",
    "df['occupation'] = le.fit_transform(df['occupation'])\n",
    "df['rating'] = [int(i/4) for i in df.rating]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wide(nn.Module):\n",
    "    def __init__(self, wide_dim, output_dim):\n",
    "        super(Wide, self).__init__()\n",
    "        self.linear = nn.Linear(wide_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        output = self.linear(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deep(nn.Module):\n",
    "    def __init__(self, embedding_input, factor_dim, layer_num, hidden_dim, output_dim):\n",
    "        super(Deep, self).__init__()\n",
    "        self.embedding_input = embedding_input\n",
    "        self.factor_dim = factor_dim\n",
    "        self.layer_num = layer_num\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        for idx, val in enumerate(self.embedding_input):\n",
    "            setattr(self, 'embedding_{}'.format(idx), nn.Embedding(val, self.factor_dim))\n",
    "        \n",
    "        self.dense_layers = self.dense()\n",
    "        \n",
    "    def dense(self):\n",
    "        dense = []\n",
    "        self.factor_dim *= len(self.embedding_input)\n",
    "        dense.append(nn.Linear(self.factor_dim, self.hidden_dim[0], bias= True))\n",
    "        dense.append(nn.ReLU())\n",
    "        for idx in range(self.layer_num-1):\n",
    "            dense.append(nn.Linear(self.hidden_dim[idx], self.hidden_dim[idx+1], bias= True))\n",
    "            dense.append(nn.ReLU())\n",
    "        dense.append(nn.Linear(self.hidden_dim[-1], self.output_dim))\n",
    "        \n",
    "        return nn.Sequential(*dense)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = [getattr(self, 'embedding_{}'.format(idx))(x[:,idx]) for idx, val in enumerate(self.embedding_input)]\n",
    "        output = torch.cat(output, 1)\n",
    "        \n",
    "        output = self.dense_layers(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeep(nn.Module):\n",
    "    def __init__(self, wide_dim, embedding_input, factor_dim, layer_num, hidden_dim, output_dim):\n",
    "        super(WideAndDeep, self).__init__()\n",
    "        \n",
    "        self.wide = Wide(wide_dim, output_dim)\n",
    "        self.deep = Deep(embedding_input, factor_dim, layer_num, hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, wide, deep):\n",
    "        wide_component = self.wide(wide)\n",
    "        deep_component = self.deep(deep)\n",
    "        return torch.sigmoid(torch.add(wide_component, deep_component))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "need_dummies = []\n",
    "for column in df.columns:\n",
    "    if df[column].nunique() > 2:\n",
    "        need_dummies.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id', 'movie_id', 'age', 'occupation']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "need_dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_columns = df.drop(columns=['rating'],axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df = pd.get_dummies(df, columns=need_dummies)\n",
    "\n",
    "for column in need_dummies:\n",
    "    wide_df[column] = df[column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>unknown</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Children</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>Drama</th>\n",
       "      <th>...</th>\n",
       "      <th>occupation_15</th>\n",
       "      <th>occupation_16</th>\n",
       "      <th>occupation_17</th>\n",
       "      <th>occupation_18</th>\n",
       "      <th>occupation_19</th>\n",
       "      <th>occupation_20</th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>age</th>\n",
       "      <th>occupation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>875</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>708</td>\n",
       "      <td>247</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>1004</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>443</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>304</td>\n",
       "      <td>506</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 2677 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rating  unknown  Action  Adventure  Animation  Children  Comedy  Crime  \\\n",
       "0           0        0       0          0          0         0       1      0   \n",
       "1           0        0       0          0          0         0       0      1   \n",
       "2           0        0       0          0          0         1       1      0   \n",
       "3           0        0       0          0          0         0       0      0   \n",
       "4           0        0       0          0          0         0       0      1   \n",
       "...       ...      ...     ...        ...        ...       ...     ...    ...   \n",
       "99995       0        0       0          0          0         0       1      0   \n",
       "99996       1        0       0          0          0         0       1      0   \n",
       "99997       0        0       0          0          0         0       0      0   \n",
       "99998       0        0       0          0          0         1       1      0   \n",
       "99999       0        0       0          0          0         0       0      0   \n",
       "\n",
       "       Documentary  Drama  ...  occupation_15  occupation_16  occupation_17  \\\n",
       "0                0      0  ...              0              0              0   \n",
       "1                0      0  ...              0              0              0   \n",
       "2                0      0  ...              0              0              0   \n",
       "3                0      1  ...              0              0              0   \n",
       "4                0      1  ...              0              0              0   \n",
       "...            ...    ...  ...            ...            ...            ...   \n",
       "99995            0      0  ...              0              0              0   \n",
       "99996            0      0  ...              0              0              0   \n",
       "99997            0      0  ...              0              0              0   \n",
       "99998            0      0  ...              0              0              0   \n",
       "99999            0      0  ...              0              0              0   \n",
       "\n",
       "       occupation_18  occupation_19  occupation_20  user_id  movie_id  age  \\\n",
       "0                  0              0              1        0         0    3   \n",
       "1                  0              0              0        1         1    2   \n",
       "2                  0              0              1        2         2    1   \n",
       "3                  0              1              0        3         3    1   \n",
       "4                  0              0              0        4         4    3   \n",
       "...              ...            ...            ...      ...       ...  ...   \n",
       "99995              1              0              0      875       173    0   \n",
       "99996              0              0              0      708       247    2   \n",
       "99997              1              0              0       37      1004    1   \n",
       "99998              0              0              0       58       443    3   \n",
       "99999              0              0              0      304       506    1   \n",
       "\n",
       "       occupation  \n",
       "0              20  \n",
       "1               6  \n",
       "2              20  \n",
       "3              19  \n",
       "4               3  \n",
       "...           ...  \n",
       "99995          18  \n",
       "99996           0  \n",
       "99997          18  \n",
       "99998           3  \n",
       "99999          13  \n",
       "\n",
       "[100000 rows x 2677 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wide_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieLensWD(Dataset):\n",
    "    def __init__(self, df, deep_columns, need_dummies):\n",
    "        self.df = df \n",
    "        self.X = df.drop(['rating'], axis=1)\n",
    "        \n",
    "        self.deep_df = self.df[deep_columns]\n",
    "        self.deep = self.deep_df.values\n",
    "        \n",
    "        self.wide_df = self.df.drop(need_dummies, axis=1)\n",
    "        self.wide = self.wide_df.to_numpy(dtype='float32')\n",
    "        \n",
    "        self.y = df['rating'].values\n",
    "        \n",
    "    # def get_wide(self):\n",
    "    #     self.wide_df = pd.get_dummies(self.X, columns=self.X.columns.drop(self.already_dummies))\n",
    "    #     self.wide = self.wide_df.to_numpy(dtype='float32')\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.wide[index], self.deep[index], self.y[index]\n",
    "    \n",
    "    def wide_dim(self):\n",
    "        return len(self.wide_df.columns)\n",
    "    \n",
    "    def deep_dims(self):\n",
    "        embedding_input = [] \n",
    "        for column in self.deep_df.columns:\n",
    "            embedding_input.append(self.deep_df[column].nunique())\n",
    "\n",
    "        return embedding_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_split(df, deep_columns, need_dummies):\n",
    "    \n",
    "    # already_dummies = [] \n",
    "\n",
    "    # for column in df.drop(columns=['rating'], axis=1).columns:\n",
    "    #     if df[column].nunique()==2:\n",
    "    #         already_dummies.append(column)\n",
    "            \n",
    "    train_X, test_X= train_test_split(df, test_size=0.2, random_state=42)\n",
    "    \n",
    "    df_dataset = MovieLensWD(df, deep_columns, need_dummies)\n",
    "\n",
    "    train_dataset_wd = MovieLensWD(train_X, deep_columns, need_dummies)\n",
    "    test_dataset_wd = MovieLensWD(test_X, deep_columns, need_dummies)\n",
    "    \n",
    "    wide_dim = df_dataset.wide_dim()\n",
    "    deep_dims = df_dataset.deep_dims()\n",
    "    \n",
    "\n",
    "    train_dataloader_wd = DataLoader(train_dataset_wd, batch_size=32, shuffle=True)\n",
    "    test_dataloader_wd = DataLoader(test_dataset_wd, batch_size=32, shuffle=True)\n",
    "\n",
    "    return train_dataloader_wd, test_dataloader_wd, wide_dim, deep_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader, test_data_loader, wide_dim, deep_dims = load_data_split(wide_df, deep_columns, need_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 0 | Loss: 0.6722539530277252 | Test Loss: 0.6602826360702515\n",
      "Epoch 1\n",
      "Epoch 1 | Loss: 0.652304321360588 | Test Loss: 0.6470566268920899\n",
      "Epoch 2\n",
      "Epoch 2 | Loss: 0.6408456965208054 | Test Loss: 0.6374604538917541\n",
      "Epoch 3\n",
      "Epoch 3 | Loss: 0.6320318527460098 | Test Loss: 0.6296242964744568\n",
      "Epoch 4\n",
      "Epoch 4 | Loss: 0.6246661900758743 | Test Loss: 0.622989042186737\n",
      "Epoch 5\n",
      "Epoch 5 | Loss: 0.6182923763275147 | Test Loss: 0.6170748338699341\n",
      "Epoch 6\n",
      "Epoch 6 | Loss: 0.6126140144586563 | Test Loss: 0.611783252620697\n",
      "Epoch 7\n",
      "Epoch 7 | Loss: 0.6074654084205627 | Test Loss: 0.6069281576156617\n",
      "Epoch 8\n",
      "Epoch 8 | Loss: 0.6027369277954101 | Test Loss: 0.6025013341903687\n",
      "Epoch 9\n",
      "Epoch 9 | Loss: 0.5983519744157791 | Test Loss: 0.5982674974441529\n",
      "Epoch 10\n",
      "Epoch 10 | Loss: 0.5942675374031067 | Test Loss: 0.5943491331100463\n",
      "Epoch 11\n",
      "Epoch 11 | Loss: 0.5904006599187851 | Test Loss: 0.5906014350891113\n",
      "Epoch 12\n",
      "Epoch 12 | Loss: 0.5867441610097885 | Test Loss: 0.587059663105011\n",
      "Epoch 13\n",
      "Epoch 13 | Loss: 0.5832741406202316 | Test Loss: 0.583755110168457\n",
      "Epoch 14\n",
      "Epoch 14 | Loss: 0.5799457152605056 | Test Loss: 0.5804967559814453\n",
      "Epoch 15\n",
      "Epoch 15 | Loss: 0.5767585837483407 | Test Loss: 0.577444973707199\n",
      "Epoch 16\n",
      "Epoch 16 | Loss: 0.5736952531218529 | Test Loss: 0.5744510927200317\n",
      "Epoch 17\n",
      "Epoch 17 | Loss: 0.5707368244290352 | Test Loss: 0.5715180006027222\n",
      "Epoch 18\n",
      "Epoch 18 | Loss: 0.567892418706417 | Test Loss: 0.5687594850540161\n",
      "Epoch 19\n",
      "Epoch 19 | Loss: 0.565127645945549 | Test Loss: 0.5660566751480103\n",
      "Epoch 20\n",
      "Epoch 20 | Loss: 0.5624518286943435 | Test Loss: 0.5634613111495972\n",
      "Epoch 21\n",
      "Epoch 21 | Loss: 0.5598542511701584 | Test Loss: 0.5609223301410675\n",
      "Epoch 22\n",
      "Epoch 22 | Loss: 0.5573289067268371 | Test Loss: 0.5584632378578186\n",
      "Epoch 23\n",
      "Epoch 23 | Loss: 0.5548709801793098 | Test Loss: 0.556065406370163\n",
      "Epoch 24\n",
      "Epoch 24 | Loss: 0.5524775419473648 | Test Loss: 0.553671624135971\n",
      "Epoch 25\n",
      "Epoch 25 | Loss: 0.5501394940972328 | Test Loss: 0.551403186416626\n",
      "Epoch 26\n",
      "Epoch 26 | Loss: 0.547856681227684 | Test Loss: 0.5491911776542664\n",
      "Epoch 27\n",
      "Epoch 27 | Loss: 0.5456348042249679 | Test Loss: 0.5470116117477417\n",
      "Epoch 28\n",
      "Epoch 28 | Loss: 0.5434537953972817 | Test Loss: 0.5448913003444672\n",
      "Epoch 29\n",
      "Epoch 29 | Loss: 0.5413239246726036 | Test Loss: 0.5427406600475311\n",
      "Epoch 30\n",
      "Epoch 30 | Loss: 0.5392386803388596 | Test Loss: 0.5407151543617249\n",
      "Epoch 31\n",
      "Epoch 31 | Loss: 0.5371821538925171 | Test Loss: 0.5386808620929718\n",
      "Epoch 32\n",
      "Epoch 32 | Loss: 0.5351874377131463 | Test Loss: 0.5367429533958435\n",
      "Epoch 33\n",
      "Epoch 33 | Loss: 0.5332217078566551 | Test Loss: 0.5348316817760468\n",
      "Epoch 34\n",
      "Epoch 34 | Loss: 0.5312943037033081 | Test Loss: 0.532952442073822\n",
      "Epoch 35\n",
      "Epoch 35 | Loss: 0.5293905762672424 | Test Loss: 0.5311353016853333\n",
      "Epoch 36\n",
      "Epoch 36 | Loss: 0.5275420043468475 | Test Loss: 0.5293083541870117\n",
      "Epoch 37\n",
      "Epoch 37 | Loss: 0.5257135282397271 | Test Loss: 0.527449701166153\n",
      "Epoch 38\n",
      "Epoch 38 | Loss: 0.5239156663417817 | Test Loss: 0.5257044997215271\n",
      "Epoch 39\n",
      "Epoch 39 | Loss: 0.5221534545898437 | Test Loss: 0.5239800122261047\n",
      "Epoch 40\n",
      "Epoch 40 | Loss: 0.5204181617736816 | Test Loss: 0.5222662543296814\n",
      "Epoch 41\n",
      "Epoch 41 | Loss: 0.518708413040638 | Test Loss: 0.5205875222206116\n",
      "Epoch 42\n",
      "Epoch 42 | Loss: 0.5170235877394677 | Test Loss: 0.5189309823989868\n",
      "Epoch 43\n",
      "Epoch 43 | Loss: 0.5153714110374451 | Test Loss: 0.5173088722229003\n",
      "Epoch 44\n",
      "Epoch 44 | Loss: 0.5137422383666038 | Test Loss: 0.5157249091148376\n",
      "Epoch 45\n",
      "Epoch 45 | Loss: 0.5121367217421532 | Test Loss: 0.5141292905330658\n",
      "Epoch 46\n",
      "Epoch 46 | Loss: 0.5105562856078147 | Test Loss: 0.5125841624736786\n",
      "Epoch 47\n",
      "Epoch 47 | Loss: 0.5089917217731476 | Test Loss: 0.5111035961151124\n",
      "Epoch 48\n",
      "Epoch 48 | Loss: 0.5074630239009857 | Test Loss: 0.5095458773612976\n",
      "Epoch 49\n",
      "Epoch 49 | Loss: 0.5059448233246804 | Test Loss: 0.5080440803050995\n",
      "Epoch 50\n",
      "Epoch 50 | Loss: 0.5044377182245254 | Test Loss: 0.5066915953159332\n",
      "Epoch 51\n",
      "Epoch 51 | Loss: 0.5029772564291954 | Test Loss: 0.5051559897899628\n",
      "Epoch 52\n",
      "Epoch 52 | Loss: 0.5015250329256058 | Test Loss: 0.5037176568984986\n",
      "Epoch 53\n",
      "Epoch 53 | Loss: 0.5000855829000473 | Test Loss: 0.5023543893814086\n",
      "Epoch 54\n",
      "Epoch 54 | Loss: 0.4986663995027542 | Test Loss: 0.5009532221317291\n",
      "Epoch 55\n",
      "Epoch 55 | Loss: 0.4972626623749733 | Test Loss: 0.49953030095100404\n",
      "Epoch 56\n",
      "Epoch 56 | Loss: 0.49588740993738173 | Test Loss: 0.49821997113227845\n",
      "Epoch 57\n",
      "Epoch 57 | Loss: 0.4945118076205254 | Test Loss: 0.49689574642181394\n",
      "Epoch 58\n",
      "Epoch 58 | Loss: 0.49317669909000394 | Test Loss: 0.4955293202400208\n",
      "Epoch 59\n",
      "Epoch 59 | Loss: 0.4918406979084015 | Test Loss: 0.49423300375938417\n",
      "Epoch 60\n",
      "Epoch 60 | Loss: 0.4905255967497826 | Test Loss: 0.4929686535835266\n",
      "Epoch 61\n",
      "Epoch 61 | Loss: 0.4892256603240967 | Test Loss: 0.4916912028312683\n",
      "Epoch 62\n",
      "Epoch 62 | Loss: 0.4879374930739403 | Test Loss: 0.4903766375541687\n",
      "Epoch 63\n",
      "Epoch 63 | Loss: 0.48666302621364593 | Test Loss: 0.4891699046611786\n",
      "Epoch 64\n",
      "Epoch 64 | Loss: 0.48540838077068327 | Test Loss: 0.48792309222221375\n",
      "Epoch 65\n",
      "Epoch 65 | Loss: 0.4841603199958801 | Test Loss: 0.4866712646484375\n",
      "Epoch 66\n",
      "Epoch 66 | Loss: 0.48293679221868513 | Test Loss: 0.48548388419151306\n",
      "Epoch 67\n",
      "Epoch 67 | Loss: 0.481715337908268 | Test Loss: 0.48429008259773254\n",
      "Epoch 68\n",
      "Epoch 68 | Loss: 0.48051399557590485 | Test Loss: 0.4831102656841278\n",
      "Epoch 69\n",
      "Epoch 69 | Loss: 0.47932265743017194 | Test Loss: 0.48195468740463254\n",
      "Epoch 70\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/input/2023-Summer-Internship-DSAIL/Recsys/WD/WD.ipynb 셀 15\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B101.101.218.32/opt/ml/input/2023-Summer-Internship-DSAIL/Recsys/WD/WD.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m wide, deep, y \u001b[39m=\u001b[39m wide\u001b[39m.\u001b[39mto(device), deep\u001b[39m.\u001b[39mto(device), y\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B101.101.218.32/opt/ml/input/2023-Summer-Internship-DSAIL/Recsys/WD/WD.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B101.101.218.32/opt/ml/input/2023-Summer-Internship-DSAIL/Recsys/WD/WD.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m pred \u001b[39m=\u001b[39m model(wide, deep)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B101.101.218.32/opt/ml/input/2023-Summer-Internship-DSAIL/Recsys/WD/WD.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(pred\u001b[39m.\u001b[39msqueeze(), y\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32))\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B101.101.218.32/opt/ml/input/2023-Summer-Internship-DSAIL/Recsys/WD/WD.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/envs/recbole/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/opt/ml/input/2023-Summer-Internship-DSAIL/Recsys/WD/WD.ipynb 셀 15\u001b[0m in \u001b[0;36mWideAndDeep.forward\u001b[0;34m(self, wide, deep)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B101.101.218.32/opt/ml/input/2023-Summer-Internship-DSAIL/Recsys/WD/WD.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, wide, deep):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B101.101.218.32/opt/ml/input/2023-Summer-Internship-DSAIL/Recsys/WD/WD.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     wide_component \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwide(wide)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B101.101.218.32/opt/ml/input/2023-Summer-Internship-DSAIL/Recsys/WD/WD.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     deep_component \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeep(deep)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B101.101.218.32/opt/ml/input/2023-Summer-Internship-DSAIL/Recsys/WD/WD.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39msigmoid(torch\u001b[39m.\u001b[39madd(wide_component, deep_component))\n",
      "File \u001b[0;32m/opt/conda/envs/recbole/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/opt/ml/input/2023-Summer-Internship-DSAIL/Recsys/WD/WD.ipynb 셀 15\u001b[0m in \u001b[0;36mWide.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B101.101.218.32/opt/ml/input/2023-Summer-Internship-DSAIL/Recsys/WD/WD.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B101.101.218.32/opt/ml/input/2023-Summer-Internship-DSAIL/Recsys/WD/WD.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear(x)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B101.101.218.32/opt/ml/input/2023-Summer-Internship-DSAIL/Recsys/WD/WD.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/opt/conda/envs/recbole/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/envs/recbole/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "model = WideAndDeep(wide_dim, deep_dims, 16, 3, [8, 4, 2], 1)\n",
    "model = model.to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=0.001)\n",
    "optimizer.param_groups[0]['capturable'] = True\n",
    "\n",
    "summary = pd.DataFrame(columns=['Epoch', 'Loss', 'Test_Loss'])\n",
    "\n",
    "for epoch in range(100):\n",
    "    \n",
    "    print(f'Epoch {epoch}')\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for wide, deep, y in train_data_loader:\n",
    "        wide, deep, y = wide.to(device), deep.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pred = model(wide, deep)\n",
    "        loss = criterion(pred.squeeze(), y.to(torch.float32))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    train_loss /= len(train_data_loader)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    test_loss = 0.0\n",
    "    for wide, deep, y in test_data_loader:\n",
    "        with torch.no_grad():\n",
    "            wide, deep, y = wide.to(device), deep.to(device), y.to(device)\n",
    "            pred = model(wide, deep)\n",
    "            loss = criterion(pred.squeeze(), y.to(torch.float32))\n",
    "            test_loss += loss.item()\n",
    "        \n",
    "    test_loss /= len(test_data_loader)\n",
    "    \n",
    "    print(f'Epoch {epoch} | Loss: {train_loss} | Test Loss: {test_loss}')\n",
    "    \n",
    "    summary = pd.concat([summary, pd.DataFrame([[epoch, train_loss, test_loss]], columns=['Epoch', 'Loss', 'Test_Loss'])])\n",
    "\n",
    "summary.to_csv('summary.csv', index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recbole",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
